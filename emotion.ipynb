{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1934845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 20:47:38.440184: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-13 20:47:38.499683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-13 20:47:39.478996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Requires: librosa\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_id = \"firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\"\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_id)\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True)\n",
    "id2label = model.config.id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59788cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, feature_extractor, max_duration=30.0):\n",
    "    audio_array, sampling_rate = librosa.load(audio_path, sr=feature_extractor.sampling_rate)\n",
    "    \n",
    "    max_length = int(feature_extractor.sampling_rate * max_duration)\n",
    "    if len(audio_array) > max_length:\n",
    "        audio_array = audio_array[:max_length]\n",
    "    else:\n",
    "        audio_array = np.pad(audio_array, (0, max_length - len(audio_array)))\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ba5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_path, model, feature_extractor, id2label, max_duration=30.0):\n",
    "    inputs = preprocess_audio(audio_path, feature_extractor, max_duration)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = id2label[predicted_id]\n",
    "    \n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368099dc",
   "metadata": {},
   "source": [
    "***wev2vec2***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9b7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ade046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1 (0.0-5.0s): angry\n",
      "Segment 2 (5.0-10.0s): surprised\n",
      "Segment 3 (10.0-15.0s): happy\n",
      "Segment 4 (15.0-20.0s): happy\n",
      "Segment 5 (20.0-25.0s): happy\n",
      "Segment 6 (25.0-26.8s): sad\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import math\n",
    "\n",
    "audio_path = \"downloads/comdey/vocals.wav\"\n",
    "\n",
    "# Load full audio\n",
    "audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "# Segment length in ms\n",
    "segment_length = 5* 1000  \n",
    "\n",
    "# Number of segments\n",
    "num_segments = math.ceil(len(audio) / segment_length)\n",
    "\n",
    "for i in range(num_segments):\n",
    "    start_time = i * segment_length\n",
    "    end_time = min((i + 1) * segment_length, len(audio))\n",
    "    \n",
    "    segment = audio[start_time:end_time]\n",
    "    segment_path = f\"/Data/deepakkumar/Subham_work/downloads/comdey/segment_{i}.wav\"\n",
    "    \n",
    "    # Save segment\n",
    "    segment.export(segment_path, format=\"wav\")\n",
    "    \n",
    "    # Predict emotion for this segment\n",
    "    predicted_emotion = predict_emotion(segment_path, model, feature_extractor, id2label)\n",
    "    print(f\"Segment {i+1} ({start_time/1000:.1f}-{end_time/1000:.1f}s): {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whats's next ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4206191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd0678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_0.wav: [[0.93654907 0.87850475 0.55000186]]\n",
      "segment_1.wav: [[0.74494344 0.7079811  0.6341447 ]]\n",
      "segment_2.wav: [[0.88779783 0.8471817  0.87374544]]\n",
      "segment_3.wav: [[0.8571902  0.79949933 0.8555167 ]]\n",
      "segment_4.wav: [[0.6314155 0.6750469 0.6169157]]\n",
      "segment_5.wav: [[0.69133055 0.62912136 0.6929021 ]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import Wav2Vec2Processor\n",
    "# from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "#     Wav2Vec2Model,\n",
    "#     Wav2Vec2PreTrainedModel,\n",
    "# )\n",
    "\n",
    "\n",
    "# class RegressionHead(nn.Module):\n",
    "#     r\"\"\"Classification head.\"\"\"\n",
    "\n",
    "#     def __init__(self, config):\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "#         self.dropout = nn.Dropout(config.final_dropout)\n",
    "#         self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "#     def forward(self, features, **kwargs):\n",
    "\n",
    "#         x = features\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.dense(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.out_proj(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "#     r\"\"\"Speech emotion classifier.\"\"\"\n",
    "\n",
    "#     def __init__(self, config):\n",
    "\n",
    "#         super().__init__(config)\n",
    "\n",
    "#         self.config = config\n",
    "#         self.wav2vec2 = Wav2Vec2Model(config)\n",
    "#         self.classifier = RegressionHead(config)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def forward(\n",
    "#             self,\n",
    "#             input_values,\n",
    "#     ):\n",
    "\n",
    "#         outputs = self.wav2vec2(input_values)\n",
    "#         hidden_states = outputs[0]\n",
    "#         hidden_states = torch.mean(hidden_states, dim=1)\n",
    "#         logits = self.classifier(hidden_states)\n",
    "\n",
    "#         return hidden_states, logits\n",
    "\n",
    "\n",
    "\n",
    "# # load model from hub\n",
    "# device = 'cpu'\n",
    "# model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "# processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "# model = EmotionModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# def process_func(\n",
    "#     x: np.ndarray,\n",
    "#     sampling_rate: int,\n",
    "#     embeddings: bool = False,\n",
    "# ) -> np.ndarray:\n",
    "#     r\"\"\"Predict emotions or extract embeddings from raw audio signal.\"\"\"\n",
    "\n",
    "#     # run through processor to normalize signal\n",
    "#     # always returns a batch, so we just get the first entry\n",
    "#     # then we put it on the device\n",
    "#     y = processor(x, sampling_rate=sampling_rate)\n",
    "#     y = y['input_values'][0]\n",
    "#     y = y.reshape(1, -1)\n",
    "#     y = torch.from_numpy(y).to(device)\n",
    "\n",
    "#     # run through model\n",
    "#     with torch.no_grad():\n",
    "#         y = model(y)[0 if embeddings else 1]\n",
    "\n",
    "#     # convert to numpy\n",
    "#     y = y.detach().cpu().numpy()\n",
    "\n",
    "#     return y\n",
    "\n",
    "# # segment_path = f\"/Data/deepakkumar/Subham_work/downloads/emotions/segment_{i}.wav\"\n",
    "    \n",
    "# import os\n",
    "# import librosa\n",
    "\n",
    "# # path where segments are stored\n",
    "# segments_dir = \"/Data/deepakkumar/Subham_work/downloads/comdey/\"\n",
    "# sampling_rate = 16000\n",
    "\n",
    "# # loop over all wav files in the folder\n",
    "# for file in sorted(os.listdir(segments_dir)):\n",
    "#     if file.endswith(\".wav\"):\n",
    "#         segment_path = os.path.join(segments_dir, file)\n",
    "\n",
    "#         # load waveform\n",
    "#         x, sr = librosa.load(segment_path, sr=sampling_rate)\n",
    "\n",
    "#         # predict VAD values\n",
    "#         vad_values = process_func(x, sr)\n",
    "\n",
    "#         print(f\"{file}: {vad_values}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24302946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_0.wav: {'hidden_states': array([[-0.00731635,  0.00544077, -0.00931627, ...,  0.00779994,\n",
      "         0.00887228,  0.00689255]], dtype=float32), 'logits': array([[0.9365496 , 0.87850547, 0.54999673]], dtype=float32)}\n",
      "segment_1.wav: {'hidden_states': array([[-0.00744769,  0.00546398, -0.00754645, ...,  0.00744426,\n",
      "         0.0088043 ,  0.00626639]], dtype=float32), 'logits': array([[0.74494237, 0.7079809 , 0.6341444 ]], dtype=float32)}\n",
      "segment_2.wav: {'hidden_states': array([[-0.00752568,  0.00799264, -0.01331591, ...,  0.00720073,\n",
      "         0.00877592,  0.0025263 ]], dtype=float32), 'logits': array([[0.8877969, 0.8471813, 0.8737451]], dtype=float32)}\n",
      "segment_3.wav: {'hidden_states': array([[-0.00757571,  0.00625499, -0.01207605, ...,  0.00766464,\n",
      "         0.00924588,  0.00633155]], dtype=float32), 'logits': array([[0.8571905, 0.7995001, 0.8555168]], dtype=float32)}\n",
      "segment_4.wav: {'hidden_states': array([[-0.00727458,  0.00568754, -0.01013857, ...,  0.00726279,\n",
      "         0.00864966,  0.00488345]], dtype=float32), 'logits': array([[0.63141614, 0.6750478 , 0.6169158 ]], dtype=float32)}\n",
      "segment_5.wav: {'hidden_states': array([[-0.00717153,  0.00792843, -0.00705739, ...,  0.00718572,\n",
      "         0.00877888,  0.00371592]], dtype=float32), 'logits': array([[0.6913291, 0.6291202, 0.6929011]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# import audeer\n",
    "# import audonnx\n",
    "\n",
    "# # model path\n",
    "# url = 'https://zenodo.org/record/6221127/files/w2v2-L-robust-12.6bc4a7fd-1.1.0.zip'\n",
    "# cache_root = audeer.mkdir('cache')\n",
    "# model_root = audeer.mkdir('model')\n",
    "\n",
    "# archive_path = audeer.download_url(url, cache_root, verbose=True)\n",
    "# audeer.extract_archive(archive_path, model_root)\n",
    "# model = audonnx.load(model_root)\n",
    "\n",
    "# # directory of audio segments\n",
    "# segments_dir = \"/Data/deepakkumar/Subham_work/downloads/comdey/\"\n",
    "# sampling_rate = 16000\n",
    "\n",
    "# # loop over all wav files\n",
    "# for file in sorted(os.listdir(segments_dir)):\n",
    "#     if file.endswith(\".wav\"):\n",
    "#         segment_path = os.path.join(segments_dir, file)\n",
    "\n",
    "#         # load waveform at 16kHz\n",
    "#         x, sr = librosa.load(segment_path, sr=sampling_rate)\n",
    "\n",
    "#         # make sure it's float32\n",
    "#         x = x.astype(np.float32)\n",
    "\n",
    "#         # run model\n",
    "#         vad = model(x, sampling_rate)\n",
    "\n",
    "#         print(f\"{file}: {vad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2cb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
